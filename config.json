{
  // Define models path
  "model_RLRF_name_or_path": "mistralai/Ministral-8B-Instruct-2410",
  "RM_name_or_path": "intfloat/e5-large-unsupervised" // The default is a 'perfect' reward model (the ranker)
  "ranker_model_name": null,

  // Define data path
  "queries_path": "queries_robust.csv",
  "data_path": "competition_history.csv",
  "RM_train_data": "",

  //Hyperparameters for RLRF pipeline
  "add_classification_layer_to_reward_model": true,
  "train_full_RM": true, // False will cause to only last layer training
  "train_RM": false,
  "relative_order": true,
  "RM_activation_function": "Sigmoid", 
  "create_perfernces_dataset": true, // If true and perfernces_dataset_path is null, new preference dataset will be created. else, no effect
  "perfernces_dataset_path": perfernces_dataset_from_ranker_train_queries_and_baseline_doc.csv",
  "value_head_needed": false, // Does the LLM need a value head for alignment 
  "use_baseline_doc_for_perfernces_dataset": true,
  "baseline_doc_column": "rejected",
  "baseline_document_path": "perfernces_dataset_robust_queries_with_ranker.csv",
  
  //Hyperparameters for RLRF model alignment. All trl library trainer config arguments can be added here.
  "HP_RLRF": {
    "learning_rate": 1e-6,
    "per_device_train_batch_size": 4,
    "gradient_accumulation_steps": 2,
    "num_train_epochs": 4,
    "beta": 0.1,
    "label_smoothing": 0.0,
    "sync_ref_model": true,
    "ref_model_mixup_alpha": 0.6,
    "ref_model_sync_steps": 256,
    "logging_strategy": "epoch",
    "use_weighting": true,
    "disable_dropout": false,
    "loss_type": "sigmoid",
    "disable_dropout": false,
    "save_steps": 10000,
    "rpo_alpha": null,
    "bf16": true,
    "fp16": false,
    "do_eval": true,
    "eval_strategy": "epoch",
    "eval_on_start": true,
    "torch_dtype": "bfloat16",
    "num_layers_to_train_RLRF": 100,
    "N_sample": 5,
    "eval_dataset_path": null,
    "eval_strategy": "epoch",
    "eval_on_start": true,
    "eval_dataset_path": "",
    "logging_dir": "",
    "model_save_path": "", //A directory
    "model_config_kwargs_RLRF": {
        "attention_dropout": 0.1
    }
  },

  //Hyperparameters for reward model training
  "HP_RM": {
    "batch_size": 10,
    "learning_rate_RM": 1.41e-5,
    "epochs": 12,
    "RM_train_loss_func": "GPTRewardLoss",
    "model_save_path": "", //A directory
    "log_path": "",
    "base_model_config_kwargs_RM": {
        "attention_probs_dropout_prob": 0,
        "hidden_dropout_prob": 0
    }
  }
}
