\## Files Overview





\### 1. `tests\_on\_competition\_log.py`



\*\*Purpose:\*\*  

Performs \*\*win-rate tests\*\* for the `RLRF\_agent` compared to the second-best agent in the competition. 



\*\*How it works:\*\*

\- Loads `competition\_history.csv` file.

\- Identifies win outcomes (1st place) for each player (0 and 1 list).

\- Identifies second best player to evaluate against.

\- Perform statistic tests



\*\*Statistical Tests:\*\*

1\. \*\*Paired t-test\*\* (one-sided):

&nbsp;  - Compares win outcomes between `RLRF\_agent` and second-best player.

&nbsp;  - Assumes paired structure over rounds.



2\. \*\*Permutation Test\*\* (paired):

&nbsp;  - Randomly swaps win labels between the two agents.

&nbsp;  - Computes empirical p-value for mean win difference.



\*\*Paths to Check/Update:\*\*

```python

file\_path

\# Path to competition history log file





\*\*Output:\*\*

\- Printed statistics:

  - Average win rates of RLRF\_agent and Second best player

  - T-test statistics and p-value

  - Permutation test p-value and observed difference









\### 2. `rank\_promotion\_from\_niv\_exp.py`



\*\*Purpose:\*\*  

Analyzes \*\*Scaled Promotion\*\* from Round 4 to Round 5 and compares how well an agent documents promoted in rank compared to human annotators.



\*\*How it works:\*\*

\- Loads `.position` files for Rounds 4 and 5.

\- Normalizes document IDs to match Round 4 ↔ Round 5.

\- Computes \*\*Scaled Promotion\*\* for each doc:

&nbsp; - `scaled\_promotion = (rank\_r4 - rank\_r5) / max\_possible\_promotion`

\- Splits into two groups: LLM (`llm\_ed\_...`) vs human annotators.

\- Performs a statistical comparison.



\*\*Statistical Test:\*\*

* \*\*Non-parametric One-sided unpaired permutation test\*\*:

&nbsp; - Our agent (`llm\_ed\_...`)

  - Human annotators



\*\*Paths to Check/Update:\*\*

```python

base\_path

\# Update base\_path to your data location.

r4\_path

r5\_path

\# Path to round 4 and 5 position files. Round 5 position should be of the results of Niv's experiment (with an LLM).



\*\*Output:\*\*

\- Printed statistics:

  - Mean scaled promotion (Agent vs human annotators)

  - p-value from permutation test







\### 3. `faith\_as\_niv\_experiment.py`



\*\*Purpose:\*\*  

Evaluates the \*\*faithfulness\*\* (OrigFaith) of documents generated by an LLM compared to documents from human annotators in a specific round (Round 5), using documents from Round 4 as a baseline.



\*\*How it works:\*\*  

\- Loads T5 TrueTeacher model.

\- For each document:

&nbsp; - Computes entailment probability between original (Round 4) and modified (Round 5) document sentences.

&nbsp; - Calculates:

&nbsp;   - \*\*RF (raw faithfulness)\*\* = % of modified sentences entailed by original.

&nbsp;   - \*\*RF\_ref (self faithfulness)\*\* = % of original sentences entailed by itself.

&nbsp;   - \*\*OrigFaith = RF / RF\_ref\*\*, capped at 1.



\*\*Statistical Test:\*\*  

\- \*\*Non-parametric Unpaired Permutation Test\*\* on OrigFaith scores between:

&nbsp; - Our agent (`llm\_ed\_...`)

&nbsp; - Human annotators 



\*\*Paths to Check/Change:\*\*

```python

base\_path 

\# Update base\_path to your data location.

\# After that line you have path to each relevant file of the result or Niv's experiment data. 





Outputs written to:

faithfulness\_llm.tsv

faithfulness\_other\_agents.tsv

faithfulness\_results\_summary.json







\### 4. `faith\_on\_Llems\_log.py`



\*\*Purpose:\*\*  

Calculates \*\*faithfulness\*\* for all players using a competition log (`competition\_history.csv`). Allows comparison of `RLRF\_env` vs. other agents across the competition timeline.



\*\*How it works:\*\*

\- Loads T5 TrueTeacher model.

\- For each player, tracks documents round-by-round.

\- For rounds > 0:

&nbsp; - Compares each modified document against the player's previous round document (or round 0 if configured).

&nbsp; - Computes \*\*OrigFaith\*\* using entailment over sentence pairs.

\- Aggregates results per player per round, and averages across all rounds.



\*\*Statistical Test:\*\*

\- No formal statistical test performed in this script.



\*\*Paths to Check/Change:\*\*

```python
competition\_log\_path
# Path to competition history log file

save\_path 

\# Path to save the json results file



\*\*Output Files:\*\*

\- Final faithfulness summary saved to save\_path





















